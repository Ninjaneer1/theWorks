{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using data from the last 20 Olympics, predict the running times of prospective Olympic sprinters.\n",
    "* You have more features (columns) than rows in your dataset.\n",
    "* Identify the most important characteristic for predicting the likelihood of being jailed before age 20.\n",
    "* Implement a filter to highlight emails that might be important to the recipient.\n",
    "* You have more than 1,000 features.\n",
    "* Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "* Your dataset dimensions are 982400x500.\n",
    "* Identify faces in an image.\n",
    "* Predict which of three flavors of ice cream will be most popular with boys versus girls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using data from the last 20 Olympics, predict the running times of prospective Olympic sprinters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any regression model could be used to find a answer. I would start with a linear regression model, because i would imagine their is like a lot of good linear relationships in the data. If not, knn regression would do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. You have more features (columns) than rows in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning models don't do so hot in this situation. So it would be best to do some dimensionality reduction with \"PCA\". Or you can use a Linear model with L1 or L2 regularization to reduce the the coefficients and help eliminate some of the features that contribute the least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Identify the most important characteristic for predicting the likelihood of being jailed before age 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is like question 2. Do some principal component analysis, and/or use coefficient values to determine the contribution of each characteritic in the analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement a filter to highlight emails that might be important to the recipient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any classification model would be capable of doing this task. Models such as, K-nearest neighbors', or Support Vector Classification are examples of classification models that could identify important emails based on characteristics of a bank of emails labeled 'important'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. You have more than 1,000 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have too many features, you need to attempt to do some demensionality reduction with PCA() of SelectKBest(), or combinning features where possible, or use linear reg with L1 and/or L2 regularization and reduce the cofficients and drop-out features that are negligable. The stats model's OLS() could be used to easily produce p-values for features to determine their weight in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Predict whether someone who adds items to their cart on a website will purchase the items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification model could be implemented in this case. A boosted decision tree model like XGBOOST classifier, would likely provide the most the accuracte results, but it cannot spit out results quickly. With a Logistic Classifier the model can more easily be tuned with respect to recall, precision, and accuracy. Recall may be particularly important in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Your dataset dimensions are 982400x500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least it has more rows than columns! I would try the further reduce the number of columns. I would try combinning them where possible, use PCA, or SelectKBest. Testing for multicolinearity will usually result in dropping some columns as well. According to SKLearn, Stochastic Gradient Descent models for regression and classification do well with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Identify faces in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN classifier and SVM classifier are both capable of this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Predict which of three flavors of ice cream will be most popular with boys versus girls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN and SVM is capable of doing this task as well, but a decision tree might be a better selection for this task, and a random forest model may do even better but would be over-kill in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
